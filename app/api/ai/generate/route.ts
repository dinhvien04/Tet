import OpenAI from 'openai';
import { NextRequest, NextResponse } from 'next/server';

const client = new OpenAI({
  baseURL: 'https://ai.megallm.io/v1',
  apiKey: process.env.MEGALLM_API_KEY,
});

export async function POST(request: NextRequest) {
  console.log('ü§ñ AI Generate API called');
  
  try {
    const { type, recipientName, traits } = await request.json();
    
    console.log('üìù Request data:', { type, recipientName, traits });

    // Validate input
    if (!type || !recipientName || !traits) {
      return NextResponse.json(
        { error: 'Missing required fields: type, recipientName, traits' },
        { status: 400 }
      );
    }

    // Validate type
    const validTypes = ['cau-doi', 'loi-chuc', 'thiep-tet'];
    if (!validTypes.includes(type)) {
      return NextResponse.json(
        { error: 'Invalid type. Must be one of: cau-doi, loi-chuc, thiep-tet' },
        { status: 400 }
      );
    }

    // Create prompts for different content types
    const prompts: Record<string, string> = {
      'cau-doi': `H√£y t·∫°o M·ªòT c√¢u ƒë·ªëi T·∫øt ng·∫Øn g·ªçn cho ${recipientName}. ƒê·∫∑c ƒëi·ªÉm: ${traits}. Ch·ªâ vi·∫øt 2 c√¢u ƒë·ªëi x·ª©ng, m·ªói c√¢u 7-8 ch·ªØ.`,
      'loi-chuc': `H√£y vi·∫øt l·ªùi ch√∫c T·∫øt ng·∫Øn g·ªçn cho ${recipientName}. ƒê·∫∑c ƒëi·ªÉm: ${traits}. Ch·ªâ 2-3 c√¢u ng·∫Øn.`,
      'thiep-tet': `H√£y t·∫°o n·ªôi dung thi·ªáp T·∫øt ng·∫Øn g·ªçn cho ${recipientName}. ƒê·∫∑c ƒëi·ªÉm: ${traits}. T·ªëi ƒëa 4-5 c√¢u.`
    };

    const model = process.env.MEGALLM_MODEL;
    
    if (!model) {
      console.error('‚ùå MEGALLM_MODEL not configured');
      return NextResponse.json(
        { error: 'MEGALLM_MODEL ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh trong .env' },
        { status: 500 }
      );
    }
    
    console.log('üöÄ Calling MegaLLM API with model:', model);
    
    const response = await client.chat.completions.create({
      model: model,
      messages: [
        { 
          role: 'system', 
          content: 'B·∫°n l√† chuy√™n gia vƒÉn h√≥a T·∫øt Vi·ªát Nam. Vi·∫øt ng·∫Øn g·ªçn, s√∫c t√≠ch, d·ªÖ hi·ªÉu.' 
        },
        { 
          role: 'user', 
          content: prompts[type] 
        }
      ],
      temperature: 0.7,
      max_tokens: 150, // Gi·∫£m xu·ªëng 150 ƒë·ªÉ ng·∫Øn h∆°n
    });

    console.log('‚úÖ MegaLLM API response received');

    const content = response.choices[0]?.message?.content || '';

    return NextResponse.json({ content });
  } catch (error: any) {
    console.error('‚ùå AI generation error:', error);
    console.error('Error details:', {
      message: error.message,
      status: error.status,
      code: error.code,
      type: error.type,
    });

    // Handle specific error types
    if (error.message?.includes('API key') || error.status === 401) {
      return NextResponse.json(
        { error: 'C·∫•u h√¨nh API key kh√¥ng h·ª£p l·ªá' },
        { status: 500 }
      );
    }

    if (error.message?.includes('quota') || error.status === 429) {
      return NextResponse.json(
        { error: 'ƒê√£ v∆∞·ª£t qu√° gi·ªõi h·∫°n s·ª≠ d·ª•ng. Vui l√≤ng th·ª≠ l·∫°i sau.' },
        { status: 429 }
      );
    }
    
    if (error.message?.includes('timeout') || error.code === 'ETIMEDOUT') {
      return NextResponse.json(
        { error: 'Request timeout. Vui l√≤ng th·ª≠ l·∫°i.' },
        { status: 504 }
      );
    }
    
    if (error.name === 'AbortError' || error.message?.includes('aborted')) {
      return NextResponse.json(
        { error: 'Request b·ªã h·ªßy. Vui l√≤ng th·ª≠ l·∫°i.' },
        { status: 499 }
      );
    }

    return NextResponse.json(
      { error: error.message || 'D·ªãch v·ª• AI t·∫°m th·ªùi g·∫∑p s·ª± c·ªë. Vui l√≤ng th·ª≠ l·∫°i.' },
      { status: 500 }
    );
  }
}
